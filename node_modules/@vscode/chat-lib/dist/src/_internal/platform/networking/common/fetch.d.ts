import { EncryptedThinkingDelta, ThinkingData, ThinkingDelta } from '../../thinking/common/thinking';
import { Response } from './fetcherService';
import { ChoiceLogProbs, FilterReason } from './openai';
export interface RequestId {
    headerRequestId: string;
    gitHubRequestId: string;
    completionId: string;
    created: number;
    serverExperiments: string;
    deploymentId: string;
}
export declare function getRequestId(response: Response, json?: any): RequestId;
export interface ICodeVulnerabilityAnnotation {
    details: {
        type: string;
        description: string;
    };
}
export interface IIPCodeCitation {
    citations: {
        url: string;
        license: string;
        snippet: string;
    };
}
export declare function isCopilotAnnotation(thing: unknown): thing is ICodeVulnerabilityAnnotation;
export declare function isCodeCitationAnnotation(thing: unknown): thing is IIPCodeCitation;
export interface ICopilotReference {
    type: string;
    id: string;
    data: Record<string, unknown>;
    metadata?: {
        display_name: string;
        display_icon?: string;
        display_url?: string;
    };
}
export interface ICopilotToolCall {
    name: string;
    arguments: string;
    id: string;
}
export interface ICopilotBeginToolCall {
    name: string;
}
/**
 * @deprecated
 */
export interface ICopilotFunctionCall {
    name: string;
    arguments: string;
}
export interface ICopilotError {
    type: string;
    code: string;
    message: string;
    agent: string;
    identifier?: string;
}
export declare function isCopilotWebReference(reference: unknown): reference is object & Record<"title", unknown> & Record<"excerpt", unknown> & Record<"url", unknown>;
export interface ICopilotWebReference {
    title: string;
    excerpt: string;
    url: string;
}
export interface ICopilotConfirmation {
    title: string;
    message: string;
    confirmation: any;
}
export interface IResponseDelta {
    text: string;
    logprobs?: ChoiceLogProbs;
    codeVulnAnnotations?: ICodeVulnerabilityAnnotation[];
    ipCitations?: IIPCodeCitation[];
    copilotReferences?: ICopilotReference[];
    copilotErrors?: ICopilotError[];
    copilotToolCalls?: ICopilotToolCall[];
    beginToolCalls?: ICopilotBeginToolCall[];
    _deprecatedCopilotFunctionCalls?: ICopilotFunctionCall[];
    copilotConfirmation?: ICopilotConfirmation;
    thinking?: ThinkingDelta | EncryptedThinkingDelta;
    retryReason?: FilterReason | 'network_error';
    /** Marker for the current response, which should be presented in `IMakeChatRequestOptions` on the next call */
    statefulMarker?: string;
}
export declare const enum ResponsePartKind {
    ContentDelta = 0,
    Content = 1,
    ToolCallDelta = 2,
    ToolCall = 3,
    Annotation = 4,
    Confirmation = 5,
    Error = 6,
    Thinking = 7,
    ThinkingDelta = 8
}
/** Part that contains incremental data added to the output */
export interface IContentDeltaResponsePart {
    kind: ResponsePartKind.ContentDelta;
    /** Part ID corresponds to the later IContentResponsePart  */
    partId: string;
    /** Incremental content chunk */
    delta: string;
}
/** Part that is emitted once the content is finished */
export interface IContentResponsePart {
    kind: ResponsePartKind.Content;
    /** Part ID of the IContentDeltaResponsePart */
    partId: string;
    /** Finalized content */
    content: string;
    /** Log probabilities, if requested */
    logProbs?: ChoiceLogProbs;
}
/** Part that contains incremental data for a tool call that's being generated */
export interface IToolCallDeltaResponsePart {
    kind: ResponsePartKind.ToolCallDelta;
    /** Part ID corresponds to the later IToolCallResponsePart  */
    partId: string;
    /** Name of the function being called */
    name: string;
    /** Arguments delta */
    delta: string;
}
/** Part that is emitted once a tool call is ready. */
export interface IToolCallResponsePart extends ICopilotToolCall {
    kind: ResponsePartKind.ToolCall;
    /** Part ID of the IToolCallDeltaResponsePart */
    partId: string;
}
/** Part that is emitted when the model wants to ask the user for confirmation. */
export interface IConfirmationResponsePart extends ICopilotConfirmation {
    kind: ResponsePartKind.Confirmation;
}
/** Part that is emitted when the model want to add annotations to a response. */
export interface IAnnotationResponsePart {
    kind: ResponsePartKind.Annotation;
    codeVulnAnnotations?: ICodeVulnerabilityAnnotation[];
    ipCitations?: IIPCodeCitation[];
    copilotReferences?: ICopilotReference[];
}
/** Part that is emitted when the model begins thinking. */
export interface IThinkingResponseDeltaPart {
    kind: ResponsePartKind.ThinkingDelta;
    /** Part ID of the IThinkingResponsePart */
    partId: string;
    /** Delta of the thinking process */
    delta: ThinkingDelta;
}
/**
 * Part that is emitted when the model finishes thinking.
 * WARN: currently CAPI never signals the end of thinking.
 */
export interface IThinkingResponsePart {
    kind: ResponsePartKind.Thinking;
    /** Part ID of IThinkingResponseDeltaPart */
    partId: string;
    /** Summary text shown to the user. */
    data: ThinkingData;
}
/** Part that is emitted when the model encounters an error. */
export interface IErrorResponsePart {
    kind: ResponsePartKind.Error;
    error: ICopilotError;
}
export type ResponsePart = IContentDeltaResponsePart | IContentResponsePart | IToolCallDeltaResponsePart | IToolCallResponsePart | IAnnotationResponsePart | IThinkingResponseDeltaPart | IThinkingResponsePart | IConfirmationResponsePart | IErrorResponsePart;
export interface FinishedCallback {
    /**
     * @param text The full concatenated text of the response
     * @param index The index of the choice to which the completion chunk belongs
     * @param delta A delta for the latest chunk
     * @returns A number to stop reading data from the server, `undefined` to continue
     */
    (text: string, index: number, delta: IResponseDelta): Promise<number | undefined>;
}
export interface OpenAiFunctionDef {
    name: string;
    description: string;
    parameters?: object;
}
export interface OpenAiFunctionTool {
    function: OpenAiFunctionDef;
    type: 'function';
}
export interface OpenAiResponsesFunctionTool extends OpenAiFunctionDef {
    type: 'function';
}
export declare function isOpenAiFunctionTool(tool: OpenAiResponsesFunctionTool | OpenAiFunctionTool | AnthropicMessagesTool): tool is OpenAiFunctionTool;
export interface AnthropicMessagesTool {
    name: string;
    description?: string;
    input_schema: {
        type: 'object';
        properties?: Record<string, any>;
        required?: string[];
    };
}
/**
 * Options for streaming response. Only set this when you set stream: true.
 *
 * @remarks Proxy has `include_usage` hard-coded to true.
 */
export type StreamOptions = {
    /**
     * If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array.
     *
     * All other chunks will also include a usage field, but with a null value. NOTE: If the stream is interrupted, you may not receive the final usage chunk which contains the total token usage for the request.
     */
    include_usage?: boolean;
};
export type Prediction = {
    type: 'content';
    content: string | {
        type: string;
        text: string;
    }[];
};
/** based on https://platform.openai.com/docs/api-reference/chat/create
 *
 * 'stream' param is not respected because we don't yet support non-streamed responses
 */
export interface OptionalChatRequestParams {
    /** Non-negative temperature sampling parameter (default 1). */
    temperature?: number;
    /** Non-negative temperature sampling parameter (default 1). */
    top_p?: number;
    /** How many parallel completions the model should generate (default 1). */
    n?: number;
    /** Whether to stream back a response in SSE format. */
    stream?: boolean;
    /** Options for streaming response. Only set this when you set stream: true. */
    stream_options?: StreamOptions;
    /** Strings that will cause the model to stop generating text. */
    stop?: string[];
    /** The maximum number of tokens to return for a completion request */
    max_tokens?: number;
    /** Likelihood of specified tokens appearing in the completion. */
    logit_bias?: number;
    presence_penalty?: number;
    frequency_penalty?: number;
    secretKey?: string;
    /** For github remote agents */
    copilot_thread_id?: string;
    copilot_skills?: string[];
    functions?: OpenAiFunctionDef[];
    function_call?: {
        name: string;
    };
    tools?: OpenAiFunctionTool[];
    /**
     * Note: 'required' is not supported
     */
    tool_choice?: 'none' | 'auto' | {
        type: 'function';
        function: {
            name: string;
        };
    };
    prediction?: Prediction;
    logprobs?: boolean;
    /** Responses API */
    previous_response_id?: string;
}
//# sourceMappingURL=fetch.d.ts.map