import { ServicesAccessor } from '../../../../../../util/vs/platform/instantiation/common/instantiation';
import { CancellationToken as ICancellationToken } from '../../../types/src';
import { Response } from '../networking';
import { TelemetryWithExp } from '../telemetry';
import { CopilotReference } from './fetch';
import { APIChoice, APILogprobs, FinishedCallback, RequestId } from './openai';
/** Gathers together many chunks of a single completion choice. */
declare class APIJsonDataStreaming {
    logprobs: number[][];
    top_logprobs: {
        [key: string]: number;
    }[][];
    text: string[];
    tokens: string[][];
    text_offset: number[][];
    copilot_annotations: CopilotAnnotations;
    tool_calls: StreamingToolCalls;
    function_call: StreamingFunctionCall;
    copilot_references: CopilotReference[];
    finish_reason?: string;
    yielded: boolean;
    append(choice: ChoiceJSON): void;
}
export declare function splitChunk(chunk: string): [string[], string];
type ModelUsage = {
    completion_tokens: number;
    prompt_tokens: number;
    total_tokens: number;
};
/**
 * A single finished completion returned from the model or proxy, along with
 * some metadata.
 */
export interface FinishedCompletion {
    solution: APIJsonDataStreaming;
    /** An optional offset into `solution.text.join('')` where the completion finishes. */
    finishOffset: number | undefined;
    /** A copilot-specific human-readable reason for the completion finishing. */
    reason: string | null;
    requestId: RequestId;
    index: number;
    model?: string;
    usage?: ModelUsage;
}
declare class StreamingToolCall {
    name?: string;
    arguments: string[];
    id?: string;
    update(toolCall: {
        type: 'function';
        id?: string;
        function: {
            name?: string;
            arguments: string;
        };
    }): void;
}
declare class StreamingToolCalls {
    private toolCalls;
    constructor();
    update(toolCallsArray: {
        type: 'function';
        id?: string;
        index?: number;
        function: {
            name?: string;
            arguments: string;
        };
    }[]): void;
    getToolCalls(): StreamingToolCall[];
}
declare class StreamingFunctionCall {
    name?: string;
    arguments: string[];
    update(functionCall: {
        name?: string;
        arguments: string;
    }): void;
}
interface FunctionCallJSON {
    name?: string;
    arguments: string;
}
interface ToolCallJSON {
    id: string;
    function: FunctionCallJSON;
    index: number;
    type: 'function';
}
export interface CopilotAnnotation {
    id: number;
    start_offset: number;
    stop_offset: number;
    details: {
        [key: string]: unknown;
    };
    citations?: {
        [key: string]: string;
    };
}
export type CopilotNamedAnnotationList = {
    [key: string]: CopilotAnnotation[];
};
export interface CopilotAnnotations {
    current: CopilotNamedAnnotationList;
    update: (annotations: CopilotNamedAnnotationList) => void;
    update_namespace: (namespace: string, annotation: CopilotAnnotation) => void;
    for: (namespace: string) => CopilotAnnotation[];
}
export declare class StreamCopilotAnnotations implements CopilotAnnotations {
    current: CopilotNamedAnnotationList;
    update(annotations: CopilotNamedAnnotationList): void;
    update_namespace(namespace: string, annotation: CopilotAnnotation): void;
    for(namespace: string): CopilotAnnotation[];
}
/** What comes back from the OpenAI API for a single choice in an SSE chunk. */
interface ChoiceJSON {
    index: number;
    /**
     * The text attribute as defined in completions streaming.
     * See https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format
     */
    text: string;
    copilot_annotations: {
        [key: string]: CopilotAnnotation[];
    };
    /**
     * The delta attribute as defined in chat streaming.
     * See https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb
     */
    delta: {
        content: string;
        copilot_annotations?: {
            [key: string]: CopilotAnnotation[];
        };
        role?: string;
        function_call?: FunctionCallJSON;
        tool_calls?: ToolCallJSON[];
    };
    finish_reason: string | null;
    logprobs?: APILogprobs;
    copilot_annotation?: CopilotNamedAnnotationList;
    copilot_references?: CopilotReference[];
}
/**
 * Processes an HTTP request containing what is assumed to be an SSE stream of
 * OpenAI API data. Yields a stream of `FinishedCompletion` objects, each as
 * soon as it's finished.
 */
export declare class SSEProcessor {
    private readonly expectedNumChoices;
    private readonly response;
    private readonly body;
    private readonly telemetryData;
    private readonly dropCompletionReasons;
    private readonly cancellationToken;
    private readonly instantiationService;
    private readonly logTarget;
    private requestId;
    private stats;
    /**
     * A key & value being here means at least one chunk with that choice index
     * has been received. A null value means we've already finished the given
     * solution and should not process incoming tokens further.
     */
    private readonly solutions;
    private constructor();
    /**
     * Creates a new instance of SSEProcessor.
     *
     * Supports dropping completions with specific finish reasons.
     * Historically, this was used to drop RAI ('content_filter') completions, instead of showing partially finished completions to the user. We've gone back and forth on this.
     */
    static create(accessor: ServicesAccessor, expectedNumChoices: number, response: Response, telemetryData: TelemetryWithExp, dropCompletionReasons?: string[], cancellationToken?: ICancellationToken): Promise<SSEProcessor>;
    /**
     * Yields finished completions as soon as they are available. The finishedCb
     * is used to determine when a completion is done and should be truncated.
     * It is called on the whole of the received solution text, once at the end
     * of the completion (if it stops by itself) and also on any chunk that has
     * a newline in it.
     *
     * Closes the server request stream when all choices are finished/truncated.
     *
     * Note that for this to work, the caller must consume the entire stream.
     * This happens automatically when using a `for await` loop, but when
     * iterating manually this needs to be done by calling `.next()` until it
     * returns an item with done = true (or calling `.return()`).
     */
    processSSE(finishedCb?: FinishedCallback): AsyncIterable<FinishedCompletion>;
    private processSSEInner;
    private asSolutionDecision;
    /** Yields the solutions that weren't yet finished, with a 'DONE' reason. */
    private finishSolutions;
    /**
     * Returns whether the cancellation token was cancelled and closes the
     * stream if it was.
     */
    private maybeCancel;
    /** Cancels the network request to the proxy. */
    private cancel;
    /** Returns whether we've finished receiving all expected solutions. */
    private allSolutionsDone;
}
export declare function prepareSolutionForReturn(accessor: ServicesAccessor, c: FinishedCompletion, telemetryData: TelemetryWithExp): APIChoice;
export {};
//# sourceMappingURL=stream.d.ts.map