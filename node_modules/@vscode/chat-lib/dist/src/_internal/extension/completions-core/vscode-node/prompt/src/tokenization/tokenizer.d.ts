import { TikTokenizer } from '@microsoft/tiktokenizer';
export declare enum TokenizerName {
    cl100k = "cl100k_base",
    o200k = "o200k_base",
    mock = "mock"
}
export declare function getTokenizer(name?: TokenizerName): Tokenizer;
export interface Tokenizer {
    /**
     * Return the length of `text` in number of tokens.
     *
     * @param text - The input text
     * @returns
     */
    tokenLength(text: string): number;
    /**
     * Returns the tokens created from tokenizing `text`.
     * @param text The text to tokenize
     */
    tokenize(text: string): number[];
    /**
     * Returns the string representation of the tokens in `tokens`, given in integer
     * representation.
     *
     * This is the functional inverse of `tokenize`.
     */
    detokenize(tokens: number[]): string;
    /**
     * Returns the tokenization of the input string as a list of strings.
     *
     * The concatenation of the output of this function is equal to the input.
     */
    tokenizeStrings(text: string): string[];
    /**
     * Return a suffix of `text` which is `n` tokens long.
     * If `text` is at most `n` tokens, return `text`.
     *
     * Note: This implementation does not attempt to return
     * the longest possible suffix, only *some* suffix of at
     * most `n` tokens.
     *
     * @param text - The text from which to take
     * @param n - How many tokens to take
     * @returns A suffix of `text`, as a `{ text: string, tokens: number[] }`.
     */
    takeLastTokens(text: string, n: number): {
        text: string;
        tokens: number[];
    };
    /**
     * Return a prefix of `text` which is `n` tokens long.
     * If `text` is at most `n` tokens, return `text`.
     *
     * Note: This implementation does not attempt to return
     * the longest possible prefix, only *some* prefix of at
     * most `n` tokens.
     *
     * @param text - The text from which to take
     * @param n - How many tokens to take
     * @returns A prefix of `text`, as a `{ text: string, tokens: number[] }`.
     */
    takeFirstTokens(text: string, n: number): {
        text: string;
        tokens: number[];
    };
    /**
     * Return the longest suffix of `text` of complete lines and is at most
     * `n` tokens long.
     * @param text - The text from which to take
     * @param n - How many tokens to take
     */
    takeLastLinesTokens(text: string, n: number): string;
}
export declare class TTokenizer implements Tokenizer {
    private readonly _tokenizer;
    constructor(_tokenizer: TikTokenizer);
    static create(encoder: TokenizerName): Promise<TTokenizer>;
    tokenize(text: string): number[];
    detokenize(tokens: number[]): string;
    tokenLength(text: string): number;
    tokenizeStrings(text: string): string[];
    takeLastTokens(text: string, n: number): {
        text: string;
        tokens: number[];
    };
    takeFirstTokens(text: string, n: number): {
        text: string;
        tokens: number[];
    };
    takeLastLinesTokens(text: string, n: number): string;
}
/** A best effort tokenizer computing the length of the text by dividing the
 * number of characters by estimated constants near the number 4.
 * It is not a real tokenizer. */
export declare class ApproximateTokenizer implements Tokenizer {
    private languageId?;
    tokenizerName: TokenizerName;
    constructor(tokenizerName?: TokenizerName, languageId?: string | undefined);
    tokenize(text: string): number[];
    detokenize(tokens: number[]): string;
    tokenizeStrings(text: string): string[];
    private getEffectiveTokenLength;
    tokenLength(text: string): number;
    takeLastTokens(text: string, n: number): {
        text: string;
        tokens: number[];
    };
    takeFirstTokens(text: string, n: number): {
        text: string;
        tokens: number[];
    };
    takeLastLinesTokens(text: string, n: number): string;
}
/** Load tokenizers on start. Export promise for to be awaited by initialization. */
export declare const initializeTokenizers: Promise<void>;
//# sourceMappingURL=tokenizer.d.ts.map