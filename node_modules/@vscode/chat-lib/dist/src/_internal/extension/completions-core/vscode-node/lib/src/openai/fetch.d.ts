import { IAuthenticationService } from '../../../../../../platform/authentication/common/authentication';
import { IInstantiationService } from '../../../../../../util/vs/platform/instantiation/common/instantiation';
import { CancellationToken as ICancellationToken } from '../../../types/src';
import { CopilotToken, ICompletionsCopilotTokenManager } from '../auth/copilotTokenManager';
import { ICompletionsLogTargetService } from '../logger';
import { Response } from '../networking';
import { ICompletionsStatusReporter } from '../progress';
import { Prompt } from '../prompt/prompt';
import { MaybeRepoInfo } from '../prompt/repository';
import { TelemetryData, TelemetryWithExp } from '../telemetry';
import { ICompletionsRuntimeModeService } from '../util/runtimeMode';
import { APIChoice, APIJsonData, RequestId } from './openai';
import { CopilotAnnotations } from './stream';
export declare enum CopilotUiKind {
    GhostText = "ghostText",
    Panel = "synthesize"
}
type BaseFetchRequest = {
    /**
     * The prompt prefix to send to the model.  Called `prompt` here for compatibility
     * with the OpenAI API.
     */
    prompt: string;
};
/**
 * Request parameters other than the prompt, which will be included in the OAI
 * API request.
 */
type CompletionFetchRequestFields = {
    /** The prompt suffix to send to the model. */
    suffix: string;
    /** Whether to stream back a response in SSE format. Always true: non streaming requests are not supported by this proxy */
    stream: boolean;
    /** Maximum number of tokens the model should generate. */
    max_tokens: number;
    /** How many parallel completions the model should generate (default 1). */
    n: number;
    /** Non-negative temperature sampling parameter (default 1). */
    temperature: number;
    /** Non-negative nucleus sampling parameter (defaults 1). */
    top_p: number;
    /** Strings that will cause the model to stop generating text. */
    stop: string[];
    /** Number of alternative tokens to include logprob data for. */
    logprobs?: number;
    /** Likelihood of specified tokens appearing in the completion. */
    logit_bias?: {
        [key: string]: number;
    };
    /** Copilot-only: NWO of repository, if any */
    nwo?: string;
    /**
     * Controls whether code citation annotations are included in the response
     * stream for non-blocking requests.
     */
    code_annotations?: boolean;
};
/** OAI API completion request, along with additional fields specific to Copilot. */
export type CompletionRequest = BaseFetchRequest & CompletionFetchRequestFields & {
    /** Copilot-only: extra arguments for completion processing. */
    extra: Partial<CompletionRequestExtra>;
};
/**
 * Completion request arguments that are Copilot-specific and don't exist in
 * the OAI API.
 */
export declare interface CompletionRequestExtra {
    /** The VSCode language ID for the file. */
    language: string;
    /**
     * If true, the proxy will trim completions to the current block/line based
     * on the force_indent and/or next_indent values.
     */
    trim_by_indentation?: boolean;
    /**
     * If set, will let the completion go on until a (non-continuation) line
     * comes through with the given indentation level.
     */
    force_indent?: number;
    /** Number of leading space or tab characters in the next non-empty line. */
    next_indent?: number;
    /**
     * For testing only: A list of completions to be used instead of calling the
     * model. The server will act as if the model returned these completions and
     * postprocess them as it normally postprocesses model responses (i.e.
     * filtering, trimming, etc.).
     */
    test_completions?: string[];
    /**
     The number of tokens (prefix)
     */
    prompt_tokens: number;
    /**
     The number of tokens (suffix)
     */
    suffix_tokens: number;
    /** Additional context to send to the model.
     * If this field is populated, then `prefix` will only contain the document prefix before the cursor.*/
    context?: string[];
}
export type PostOptions = Partial<CompletionFetchRequestFields>;
export declare function getRequestId(response: Response): RequestId;
export interface CopilotError {
    type: string;
    code: string;
    message: string;
    identifier: string;
}
export interface CopilotConfirmation {
    type: string;
    title: string;
    message: string;
    confirmation: Record<string, unknown>;
}
export interface CopilotReference {
    type: string;
    id: string;
    data: Record<string, unknown>;
}
export interface RequestDelta {
    text: string;
    index?: number;
    requestId?: RequestId;
    annotations?: CopilotAnnotations;
    copilotErrors?: CopilotError[];
    copilotConfirmation?: CopilotConfirmation;
    copilotReferences?: CopilotReference[];
    getAPIJsonData?: () => APIJsonData;
    finished?: boolean;
    telemetryData?: TelemetryWithExp;
}
export interface SolutionDecision {
    yieldSolution: boolean;
    continueStreaming: boolean;
    finishOffset?: number;
}
type FinishedCallbackResult = Promise<SolutionDecision | number | undefined> | SolutionDecision | number | undefined;
/**
 * Takes a (part of a) completion resolves to the offset of the end of the
 * block, or undefined if the block is not yet finished.
 */
export interface FinishedCallback {
    (text: string, delta: RequestDelta): FinishedCallbackResult;
}
interface InternalFetchParams {
    prompt: Prompt;
    engineModelId: string;
    uiKind: CopilotUiKind;
    ourRequestId: string;
    headers?: CompletionHeaders;
}
/**
 * Interface for the parameters passed to `fetchAndStreamCompletions` and `fetchWithParameters` wrappers,
 * which then turn them into a `CompletionRequest` to be sent with `fetchWithInstrumentation`.
 */
export interface CompletionParams extends InternalFetchParams {
    repoInfo: MaybeRepoInfo;
    languageId: string;
    count: number;
    requestLogProbs?: boolean;
    postOptions?: PostOptions;
    extra: Partial<CompletionRequestExtra>;
}
/**
 * Interface for the parameters passed to `fetchSpeculationWithParameters`,
 * which then turns them into a `SpeculationCompletionRequest` object to be sent with `fetchWithInstrumentation`.
 */
export interface SpeculationFetchParams extends InternalFetchParams {
    speculation: string;
    stops: string[] | null;
}
export declare const ICompletionsOpenAIFetcherService: import("../../../../../../util/common/services").ServiceIdentifier<ICompletionsOpenAIFetcherService>;
export interface ICompletionsOpenAIFetcherService {
    readonly _serviceBrand: undefined;
    fetchAndStreamCompletions(params: CompletionParams, baseTelemetryData: TelemetryWithExp, finishedCb: FinishedCallback, cancellationToken?: ICancellationToken): Promise<CompletionResults | CompletionError>;
}
/** An interface to abstract away the network request to OpenAI, allowing for
 * fake or mock implementations. It's deliberately injected relatively high
 * in the call stack to avoid having to reconstruct some of the lower-level details
 * of the OpenAI API.
 */
export declare abstract class OpenAIFetcher implements ICompletionsOpenAIFetcherService {
    _serviceBrand: undefined;
    /**
     * Sends a request to the code completion endpoint.
     */
    abstract fetchAndStreamCompletions(params: CompletionParams, baseTelemetryData: TelemetryWithExp, finishedCb: FinishedCallback, cancellationToken?: ICancellationToken): Promise<CompletionResults | CompletionError>;
}
export interface CompletionResults {
    type: 'success';
    choices: AsyncIterable<APIChoice>;
    getProcessingTime(): number;
}
export type CompletionError = {
    type: 'failed';
    reason: string;
} | {
    type: 'canceled';
    reason: string;
};
export type CompletionHeaders = {
    /** For speculation only**/
    Host?: string;
    Connection?: string;
    'X-Copilot-Async'?: string;
    'X-Copilot-Speculative'?: string;
};
export declare function sanitizeRequestOptionTelemetry(request: Partial<CompletionRequest>, telemetryData: TelemetryWithExp, topLevelKeys: string[], // top-level properties to exclude from standard telemetry
extraKeys?: (keyof CompletionRequestExtra)[]): void;
export declare function postProcessChoices(choices: AsyncIterable<APIChoice>): AsyncIterable<APIChoice>;
export declare const CMDQuotaExceeded = "github.copilot.completions.quotaExceeded";
export declare class LiveOpenAIFetcher extends OpenAIFetcher {
    #private;
    private readonly instantiationService;
    private readonly runtimeModeService;
    private readonly logTargetService;
    private readonly copilotTokenManager;
    private readonly statusReporter;
    private readonly authenticationService;
    constructor(instantiationService: IInstantiationService, runtimeModeService: ICompletionsRuntimeModeService, logTargetService: ICompletionsLogTargetService, copilotTokenManager: ICompletionsCopilotTokenManager, statusReporter: ICompletionsStatusReporter, authenticationService: IAuthenticationService);
    fetchAndStreamCompletions(params: CompletionParams, baseTelemetryData: TelemetryWithExp, finishedCb: FinishedCallback, cancel?: ICancellationToken): Promise<CompletionResults | CompletionError>;
    private createTelemetryData;
    fetchWithParameters(endpoint: string, params: CompletionParams, copilotToken: CopilotToken, baseTelemetryData: TelemetryWithExp, cancel?: ICancellationToken): Promise<Response | 'not-sent'>;
    handleError(statusReporter: ICompletionsStatusReporter, telemetryData: TelemetryData, response: Response, copilotToken: CopilotToken): Promise<CompletionError>;
}
export {};
//# sourceMappingURL=fetch.d.ts.map